---
title: "report"
author: "Fang"
date: "1/28/2018"
output: md_document
---

# Online News Popularity

## Introduction
One challenging part for social media is to attract readers. And one of the most challenging part is to predict the number of shares in social networks (popularity). Many factors affect the popularity and many may not. My goal is to select most import factors and model to make dataset easier and prediction more accurate.

## Dataset
I used the Online News Popularity Data Set from UCI Machine Learning library [here](http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)
. The dataset contains *onlinenewspopularity.csv* with 61 features and 39644 observations.Features information:
```
0. url: URL of the article (non-predictive) 
1. timedelta: Days between the article publication and the dataset acquisition (non-predictive) 
2. n_tokens_title: Number of words in the title 
3. n_tokens_content: Number of words in the content 
4. n_unique_tokens: Rate of unique words in the content 
5. n_non_stop_words: Rate of non-stop words in the content 
6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content 
7. num_hrefs: Number of links 
8. num_self_hrefs: Number of links to other articles published by Mashable 
9. num_imgs: Number of images 
10. num_videos: Number of videos 
11. average_token_length: Average length of the words in the content 
12. num_keywords: Number of keywords in the metadata 
13. data_channel_is_lifestyle: Is data channel 'Lifestyle'? 
14. data_channel_is_entertainment: Is data channel 'Entertainment'? 
15. data_channel_is_bus: Is data channel 'Business'? 
16. data_channel_is_socmed: Is data channel 'Social Media'? 
17. data_channel_is_tech: Is data channel 'Tech'? 
18. data_channel_is_world: Is data channel 'World'? 
19. kw_min_min: Worst keyword (min. shares) 
20. kw_max_min: Worst keyword (max. shares) 
21. kw_avg_min: Worst keyword (avg. shares) 
22. kw_min_max: Best keyword (min. shares) 
23. kw_max_max: Best keyword (max. shares) 
24. kw_avg_max: Best keyword (avg. shares) 
25. kw_min_avg: Avg. keyword (min. shares) 
26. kw_max_avg: Avg. keyword (max. shares) 
27. kw_avg_avg: Avg. keyword (avg. shares) 
28. self_reference_min_shares: Min. shares of referenced articles in Mashable 
29. self_reference_max_shares: Max. shares of referenced articles in Mashable 
30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable 
31. weekday_is_monday: Was the article published on a Monday? 
32. weekday_is_tuesday: Was the article published on a Tuesday? 
33. weekday_is_wednesday: Was the article published on a Wednesday? 
34. weekday_is_thursday: Was the article published on a Thursday? 
35. weekday_is_friday: Was the article published on a Friday? 
36. weekday_is_saturday: Was the article published on a Saturday? 
37. weekday_is_sunday: Was the article published on a Sunday? 
38. is_weekend: Was the article published on the weekend? 
39. LDA_00: Closeness to LDA topic 0 
40. LDA_01: Closeness to LDA topic 1 
41. LDA_02: Closeness to LDA topic 2 
42. LDA_03: Closeness to LDA topic 3 
43. LDA_04: Closeness to LDA topic 4 
44. global_subjectivity: Text subjectivity 
45. global_sentiment_polarity: Text sentiment polarity 
46. global_rate_positive_words: Rate of positive words in the content 
47. global_rate_negative_words: Rate of negative words in the content 
48. rate_positive_words: Rate of positive words among non-neutral tokens 
49. rate_negative_words: Rate of negative words among non-neutral tokens 
50. avg_positive_polarity: Avg. polarity of positive words 
51. min_positive_polarity: Min. polarity of positive words 
52. max_positive_polarity: Max. polarity of positive words 
53. avg_negative_polarity: Avg. polarity of negative words 
54. min_negative_polarity: Min. polarity of negative words 
55. max_negative_polarity: Max. polarity of negative words 
56. title_subjectivity: Title subjectivity 
57. title_sentiment_polarity: Title polarity 
58. abs_title_subjectivity: Absolute subjectivity level 
59. abs_title_sentiment_polarity: Absolute polarity level 
60. shares: Number of shares (target)
```

##Analysis
I cut my data set into 2 parts. 10% for training and 90% for testing, since 10% dataset have more than 3000 observations, which is enough. It has one response, which is the number of shares. I'll try to reduce the number of other features and fit a model to increase the accuracy of prediction.

```{r}
data <- read.csv("data/onlinenewspopularity.csv")
head(data)
```
I cut the column of 'url' and 'shares'. Therefore, we have data summary as
```
Number of training examples: 3964
Number of features: 59
```
I fit *sklearn.linear_model.Ridge* model to record the training score and validation score for initial dataset.
```
Training score:   0.0421
Validation score: -0.344
```

### Feature selection
I wrote *ForwardSelection()* function to search the best features for a given dataset. 
```
fs = ForwardSelection(Ridge())
fs.fit(Xtrain,ytrain)
print('features: {}'.format(fs.ftr_))
print('score: {}'.format(fs.score_))
```
```
features: [8, 31, 1, 9, 7, 14, 16, 39, 17, 19, 50, 45, 0, 2, 15, 42, 12, 10, 4, 33, 53, 47, 46, 48, 18, 56, 51]
score: -0.07203325084218193
```
We got the column number for the feature and our score is -0.07, which is higher than the initial dataset.

![](results/feature.png)

This plot interprate the relationship between error and the number of feature selection. When number of features is greater than 27, the error will not change anymore.

### Model selection
After the feature selection, we will do model selection. I wrote a funtion *err_plot()* to output the error rate and plot. 

For *Ridge* model:

![](results/ridge.png)

for *Lasso* model:

![](results/lasso.png)

for *ElasticNet* model:

![](results/elasticnet.png)

We find the best model is ElasticNet with alpha = 0.316.

## Conclusion:

Let's compare results:
For initial dataset:
```
Training score:   0.0421
Validation score: -0.344
```

For reduced dataset:
```
Training score:   0.0114
Validation score: 0.00263
```
 We have higher scores and lower number of features. 


